# ðŸ›  Proyecto Airflow + MinIO + ML Pipeline

Este proyecto orquesta un flujo de procesamiento de datos y entrenamiento de un modelo SVM usando **Apache Airflow**, con almacenamiento de datos en **MinIO (S3 compatible)**. Todo estÃ¡ dockerizado y configurado para correr automÃ¡ticamente.

## Integrantes
- a1822 Cristian Patricio Salinas Talamilla
- a1812 Ezequiel Eduardo Maudet
- a1811 Marcelo AdriÃ¡n MÃ¡s Valdecantos
- a1806 Lucas Fajardo
- a1826 Sebastian Carreras

## ðŸ“¦ Estructura del Proyecto

```
.
â”œâ”€â”€ dags/                      # DAGs de Airflow
â”œâ”€â”€ scripts/                   # Scripts auxiliares (opcional)
â”œâ”€â”€ logs/                      # Carpeta ignorada por Git (Airflow logs)
â”œâ”€â”€ datalake/                  # Carpeta local (opcional) para almacenamiento temporal
â”œâ”€â”€ docker-compose.yml         # DefiniciÃ³n de servicios
â””â”€â”€ .gitignore                 # Ignora logs, datalake, etc.
```

## ðŸ” Flujo del DAG principal

1. **`crear_bucket_minio`**  
   ConexiÃ³n automÃ¡tica a MinIO vÃ­a `S3Hook`. Verifica si existe el bucket `datalake`, y si no, lo crea.

2. **`check_minio_connection`**  
   Escribe un archivo `conexion_exitosa.txt` en el bucket `datalake` para confirmar la conexiÃ³n.

3. **`descargar_csv`**  
   Descarga un archivo CSV desde Google Drive a `/opt/airflow/datalake/df_merged.csv`. Este es el dataset usado para la prediccion de polvo de la materia AdM 1. 

4. **`mostrar_head`**  
   Lee el CSV con `pandas`, convierte la columna `date` a datetime, elimina columnas no deseadas y guarda un backup en formato JSON.
   Hace un preproceso de los datos para que queden listos para ingresar el modelo. Respalda el dataset como un JSON.

6. **`split_dataset`**  
   - Convierte la columna `Date` a `timestamp`.
   - Elimina columnas no relevantes para el modelo.
   - Realiza un `train_test_split` y guarda `X_train`, `X_test`, `y_train`, `y_test` como JSON en la carpeta local de respaldo.

7. **`svm_modeling`**  
   - Escala los datos con `StandardScaler`.
   - Entrena un `SVC(kernel='linear')`.
   - Hace `cross_val_score` y muestra la matriz de confusiÃ³n.
   - Todo se loguea en el contenedor Airflow.

## ðŸŒ MinIO (S3 compatible)

- Se levanta en el contenedor `minio`.
- Console: [http://localhost:9001](http://localhost:9001)
- API: [http://localhost:9000](http://localhost:9000)
- Usuario/ContraseÃ±a: `minioadmin / minioadmin`
- Bucket usado: `datalake`

Airflow se conecta a MinIO automÃ¡ticamente con:

```yaml
environment:
  AIRFLOW_CONN_MINIO_S3: s3://minioadmin:minioadmin@minio:9000/?endpoint_url=http%3A%2F%2Fminio%3A9000
```

## ðŸ§¹ Git

- Se ignoran los logs y archivos temporales:
```
logs/
datalake/
```

Para limpiar logs ya subidos:

```bash
git rm -r --cached logs/
echo "logs/" >> .gitignore
git commit -m "Ignorar carpeta logs"
git push
```

## ðŸš€ CÃ³mo levantar todo

```bash
docker-compose up -d
```

Una vez levantado, accedÃ© a Airflow:
- [http://localhost:8080](http://localhost:8080)
- Usuario: `airflow`, Password: `airflow` (segÃºn imagen oficial)

## Pasos a Seguir
   - Lograr respaldar los archivos en el bucket conectado usando S3.hook()
   - Probar otros modelos
   - Respaldar en el bucket un modelo en plk
   - Tomar con una nueva tarea ese modelo creado y probarlo con un set de datos al azar para hacer un predict
